# Why fiber?

Fiber optic cables are currently the best means for providing internet to everyone. As we in the US spend money on infrastructure, we should make sure to invest it wisely in fiber optic internet. Fiber technology has been <a href="https://en.wikipedia.org/wiki/Fiber-optic_communication#History">commercially available for 46 years</a> and current commercial technology can reach <a href="https://en.wikipedia.org/wiki/Fiber-optic_communication#Wavelength-division_multiplexing">1.6 Tbit/s (or 1600 Gigabit/s) over a single fiber</a>. In a lab setting, fiber has achieved up to <a href="https://en.wikipedia.org/wiki/Fiber-optic_communication#Standard_fibre_cables">101.7 Tbit/s</a> over an existing commercial fiber strand. No other current technology, wireless or wireline, can match these speeds. See below for why speed matters and why fiber to the premises (home, business or institution) is a worthwhile investment.

# Data Basics

Computers do computation over data. Think of computation as transforming data or moving it from one place to another to cause something to occur. For a formal definition, see [Turing completeness](). The smallest element of data is a bit. It represents a single boolean value meaning it has two possible states (0 or 1, yes or no). Computer programs and peripherals, like TVs and speakers, give meaning to these bits. Usually many bits are needed to represent something. For numbers, 0-255 for example, it takes 8 bits, each bit is "worth" twice the previous bit (4 + 2 + 1 = 7 for three bits). Since it is so common to require more than a single bit, 8 bits are often bundled into bytes. In fact, computers nowadays tend to compute over 64 bits (8 bytes) at once, hence the term 64 bit computers. This distinction between bits and bytes is importand because data rates for internet connectivity are usually given in bits per second, rather than bytes. When you look at a file size on your computer, it is in bytes (not bits.)

One of the main ways we use the internet is to watch (Netflix, Hulu, etc.) or broadcast video (Skype, Zoom, etc.). As video quality increases, the amount of bits needed to store that video also increases. This is one of the main reasons we want faster internet now than we did a decade ago. Video technology continues to improve. Let's dig a bit into the basics of video file size. (We'll ignore audio for simplicity.)

A video file is a series of pictures and each picture is a set of pixels. Pixel is short for "picture element". Similar to bit, it is the smallest unit in a picture. A TV or monitor features three color elements per pixel so that it can change the color of that element. Pixels tend to take 3 bytes to represent how much red, green and blue to combine to produce a particular color. This is known as 8-bit color. Newer TVs are increasing this to 10 or even 12 bits per color. We'll ignore those here (but keep this in mind when picking your internet.)

Videos tend to have a resolution. HD and 4K are currently the most common. HD pictures are 1920 pixels wide by 1080 pixels high. 4k is HD doubled in each dimension and therefore has four times as many pixels. HD pictures are 2,073,600 pixels. 4k pictures are 8,294,400. So for each picture (also known as a frame) in a video file, you are storing at least 2,073,600 * 3 bytes = 6,220,800 bytes. That's a lot of bytes!

As file sizes grew, we've started using prefixes to designate larger collections of bytes. A kilobyte is 1000 bytes. A megabyte is 1000 * 1000 bytes. Computers are built on binary numbers though which are based on two option. As a result, it's easier for computers to divide by 1024 because it is a power of two (2 ** 10). Therefore, we have power of two prefixes: a kibibyte is 1024 bytes and a mibibyte is 1024 * 1024 = 1,048,576 bytes. This distinction isn't always made correctly though. Mibibytes are often referred to as megabytes. Only storage sizes of hard drives on the box are usually quoted in actual gigabytes because the number is higher. So, we'll do our math in "megabytes" meaning 1,048,576 bytes. Going back to our HD picture, the 6,220,800 bytes is 5.93 "megabytes".

Video isn't just a single picture though. In classic movies it's 24 frames per second. On US TV (NTSC) it is ~30 frames per second. Newer TVs can do 60, 120 or even 240 frames per second. This is known as the refresh rate and is usually measured in Hertz (Hz for short.) So, to show one second of HD 60 Hz video it is 6,220,800 bytes * 60 frames = 373,248,000 bytes per second or 355.95 "megabytes" per second. Video compression techniques can reduce the amount of data need somewhat but the ideal quality isn't compressed at all.

Doing this computation per second can help us understand data rates. Data rates are the rate at which data is being used to produce an effect. Above is a video data rate and rates are common in audio as well. When it comes to your internet, rates are almost always given in bits rather than bytes. The bitrate of the video is 8 times larger than the byte rate, 2847.65 megabits per second. If your internet is slower than that, you'll quickly run out of bits and the video will stop. In reality, video streaming services adjust the quality of the video to ensure that your internet can keep up as time goes by. When things get really dire, the service will buffer video before starting playback. Buffering gives your internet a head start to download the video before starting to consume bits with playback.

Suppose your video player wants to buffer the first gigabyte (1024 megabytes) of the video as a head start. How long would it take to buffer on a 32 megabit per second connection? (This is higher than the FCC's definition of "broadband" which is 25 megabits per second.) First, this 32 megabits is really only 4 megabytes per second. It would take 1024 megabytes / 4 megabytes per second = 256 seconds or 4 minutes 16 seconds to download that first gigabyte. On a gigabit connection similar to the common top tier of fiber optic providers (commonly ~$85 per month) that is 256 megabytes per second. With that connection, the buffering would take just 1024 megabytes / 256 megabytes per second = 4 seconds to download. This bit rate math applies to all files you upload or download.

Download rates are often higher than upload rates. Download rates matter when you are consuming bits. Upload rates matter when you are creating bits. For example, when you are in a video chat, your camera is creating video bits that are then uploaded to the other people you are talking to. If you can't upload all of bits you're creating, the video chat software will reduce your quality to others by dropping the resolution, quality and frame rate. If the upload bit rate changes, it can cause choppy video where it looks good for a bit and then halts and stutters.

Upload rate also matters when you are sharing other sorts of files. Uploading a 1 gigabyte file on a gigabit connection will take the same 4 seconds it took to download it because gigabit connections are typically "symmetric" which means they provide the same bit rate for both upload as it does for download. This is almost always *not* the case for cable, DSL, and satellite internet. Usually more of the data capability of the link is dedicated to download than upload. Picture a 4 lane roadway where one direction is three lanes and the other is a single lane. Symmetric connections (often fiber) would have the same number of lanes in each direction.

# Network Basics

Networks are made up of computers connected by a series of data links. Each link has a certain capacity, just like a gigabit connection can do one gigabit of data in each direction. However, in a network there are usually multiple ways data can be sent between two computers in the network. In the simplest example, two gigabit links between two computers would provide a total data rate of two gigabits in each direction. Network theory calls this the maximum flow. If two computers (your laptop and your phone for example) share a link they'd share this gigabit capacity. This is why internet download rates are commonly specified for how many people live in a household, the internet capacity is split between them.

The limiting factor for internet speed in a household is almost always the connection that leaves the household to the internet service provider (ISP). Let's talk over the common network.

# Phone -> Wi-Fi router

Within a household, most devices are connected to the internet via WiFi. WiFi is a data link that transmits radio signals between your device to the WiFi router. The signals travel through everything between them. It's usually air but may include walls, floors, plants, pets or people. How and through what the signals travel through between your device and the WiFi router dictates the signal quality of that connection. The impact of travelling through different things varies based on frequency. 2.4 ghz radio travels better than 5 ghz for example. Lower signal quality reduces the rate bits can be sent because more bits are wasted to ensure that they are transferred correctly. WiFi provides freedom to move around at the cost of varying signal quality (closer to the router is better because it has less to travel through). Furthermore, WiFi signals aren't directed specifically between devices. Each device connected to a router shares data rate with every other device on the same radio channel (not just that router.) This is known as "crowding" and causes WiFi to degrade in high density areas. Modern protocols use more radio channels and skip around to find the least crowded channels.

Common WiFi is 802.11ac which can do 500 megabits per second minimum (devices starting in 2013 and 2014) up to 3 gigabits or so (shared amongst all devices). The older 802.11n spec can do 72 megabits in a single stream and up to 600 megabits in aggregate (total shared amongst devices.) Even older tech are 802.11a, b, and g. If you have a router that can do only do any of these, you should upgrade them first. They have max data rates of 54 megabits or less.

The next generation of Wi-Fi, called Wi-Fi 6 or 802.11ax, can do up to 9.6 gigabits per second. It also features better protocols for sharing the link between devices so they each can get more bits through.

Overall, if your Wi-Fi is 802.11ac or Wi-Fi 6, you probably aren't limiting your internet speeds with your Wi-Fi.

# Wi-Fi router -> "Modem"
From the Wi-Fi router the bits then travel to the "modem" via an Ethernet cable. Two other variations are that the modem is also a wifi router or a WiFi mesh uses more WiFi to talk to a device that is plugged in. WiFi mesh can optimize that link a bit because the mesh components don't usually move.

So, Ethernet is by far the most common way bits get from a Wi-Fi router to a "modem". Ethernet signals travel over copper cables that are usually designated CAT5 or CAT6. The category (CAT) designates how well signals are transmitted through the copper of the cable. Most cables are labeled with their category. CAT 5e cables can do up to 2.5 gigabits per second. Regular CAT 5 can do up to 1 gigabit ethernet. Common Ethernet speeds are 10, 100, and 1000 megabits per second. Often routers and switches will have lights to tell you if a link is 100 or 1000 megabits. One or more routers and switches may be between your Wi-Fi router and the "modem". The slowest link between them will dictate the maximum speed because there is usually only one path between the Wi-Fi router and "modem".

What "modem" means varies depending on the type of internet service you have.

# Cable modem -> 

# DSL modem -> 

# Fiber "modem" -> 
